public final class org.apache.mahout.vectorizer.HighDFWordsPruner extends java.lang.Object
{
    public static final java.lang.String STD_CALC_DIR;
    public static final java.lang.String MAX_DF;
    public static final java.lang.String MIN_DF;

    private void <init>()
    {
        org.apache.mahout.vectorizer.HighDFWordsPruner r0;

        r0 := @this: org.apache.mahout.vectorizer.HighDFWordsPruner;

        specialinvoke r0.<java.lang.Object: void <init>()>();

        return;
    }

    public static void pruneVectors(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, long, long, org.apache.hadoop.conf.Configuration, org.apache.mahout.common.Pair, float, boolean, int) throws java.io.IOException, java.lang.InterruptedException, java.lang.ClassNotFoundException
    {
        org.apache.hadoop.fs.Path r0, r1, r2, r7, $r14;
        long l0, l1;
        org.apache.hadoop.conf.Configuration r3, $r13;
        org.apache.mahout.common.Pair r4;
        float f0;
        boolean z0, $z1;
        int i2, $i3, i4;
        java.util.Iterator r6;
        java.util.ArrayList $r8;
        java.util.List $r10;
        java.lang.Object $r11, $r15;
        org.apache.hadoop.fs.Path[] $r12;
        java.lang.StringBuilder $r16, $r17, $r18;
        java.lang.String $r19;

        r0 := @parameter0: org.apache.hadoop.fs.Path;

        r1 := @parameter1: org.apache.hadoop.fs.Path;

        r2 := @parameter2: org.apache.hadoop.fs.Path;

        l0 := @parameter3: long;

        l1 := @parameter4: long;

        r3 := @parameter5: org.apache.hadoop.conf.Configuration;

        r4 := @parameter6: org.apache.mahout.common.Pair;

        f0 := @parameter7: float;

        z0 := @parameter8: boolean;

        i2 := @parameter9: int;

        i4 = 0;

        $r8 = new java.util.ArrayList;

        specialinvoke $r8.<java.util.ArrayList: void <init>()>();

        $r11 = virtualinvoke r4.<org.apache.mahout.common.Pair: java.lang.Object getSecond()>();

        $r10 = (java.util.List) $r11;

        r6 = interfaceinvoke $r10.<java.util.List: java.util.Iterator iterator()>();

     label1:
        $z1 = interfaceinvoke r6.<java.util.Iterator: boolean hasNext()>();

        if $z1 == 0 goto label2;

        $r15 = interfaceinvoke r6.<java.util.Iterator: java.lang.Object next()>();

        r7 = (org.apache.hadoop.fs.Path) $r15;

        $r14 = new org.apache.hadoop.fs.Path;

        $r17 = new java.lang.StringBuilder;

        specialinvoke $r17.<java.lang.StringBuilder: void <init>()>();

        $r16 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("partial-");

        $i3 = i4;

        i4 = i4 + 1;

        $r18 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i3);

        $r19 = virtualinvoke $r18.<java.lang.StringBuilder: java.lang.String toString()>();

        specialinvoke $r14.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>(r2, $r19);

        interfaceinvoke $r8.<java.util.List: boolean add(java.lang.Object)>($r14);

        staticinvoke <org.apache.mahout.vectorizer.HighDFWordsPruner: void pruneVectorsPartial(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,long,long,org.apache.hadoop.conf.Configuration)>(r0, $r14, r7, l0, l1, r3);

        goto label1;

     label2:
        staticinvoke <org.apache.mahout.vectorizer.HighDFWordsPruner: void mergePartialVectors(java.lang.Iterable,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,float,boolean,int)>($r8, r1, r3, f0, z0, i2);

        $r13 = new org.apache.hadoop.conf.Configuration;

        specialinvoke $r13.<org.apache.hadoop.conf.Configuration: void <init>(org.apache.hadoop.conf.Configuration)>(r3);

        $r12 = newarray (org.apache.hadoop.fs.Path)[1];

        $r12[0] = r2;

        staticinvoke <org.apache.mahout.common.HadoopUtil: void delete(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path[])>($r13, $r12);

        return;
    }

    private static void pruneVectorsPartial(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, long, long, org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.lang.InterruptedException, java.lang.ClassNotFoundException
    {
        org.apache.hadoop.fs.Path r0, r1, r2;
        long l0, l1;
        org.apache.hadoop.conf.Configuration r3, $r6;
        org.apache.hadoop.mapreduce.Job r5;
        boolean z0;
        java.net.URI $r7;
        java.lang.StringBuilder $r8, $r9, $r10, $r11, $r12;
        java.lang.String $r13, $r15;
        org.apache.hadoop.fs.Path[] $r14;
        java.lang.IllegalStateException $r16;

        r0 := @parameter0: org.apache.hadoop.fs.Path;

        r1 := @parameter1: org.apache.hadoop.fs.Path;

        r2 := @parameter2: org.apache.hadoop.fs.Path;

        l0 := @parameter3: long;

        l1 := @parameter4: long;

        r3 := @parameter5: org.apache.hadoop.conf.Configuration;

        $r6 = new org.apache.hadoop.conf.Configuration;

        specialinvoke $r6.<org.apache.hadoop.conf.Configuration: void <init>(org.apache.hadoop.conf.Configuration)>(r3);

        virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String)>("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization,org.apache.hadoop.io.serializer.WritableSerialization");

        virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: void setLong(java.lang.String,long)>("max.df", l0);

        virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: void setLong(java.lang.String,long)>("min.df", l1);

        $r7 = virtualinvoke r2.<org.apache.hadoop.fs.Path: java.net.URI toUri()>();

        staticinvoke <org.apache.hadoop.filecache.DistributedCache: void addCacheFile(java.net.URI,org.apache.hadoop.conf.Configuration)>($r7, $r6);

        r5 = staticinvoke <org.apache.mahout.common.HadoopUtil: org.apache.hadoop.mapreduce.Job prepareJob(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,java.lang.Class,java.lang.Class,java.lang.Class,java.lang.Class,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration)>(r0, r1, class "org/apache/hadoop/mapreduce/lib/input/SequenceFileInputFormat", class "org/apache/hadoop/mapreduce/Mapper", null, null, class "org/apache/mahout/vectorizer/pruner/WordsPrunerReducer", class "org/apache/hadoop/io/Text", class "org/apache/mahout/math/VectorWritable", class "org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat", $r6);

        $r8 = new java.lang.StringBuilder;

        specialinvoke $r8.<java.lang.StringBuilder: void <init>()>();

        $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(": Prune Vectors: input-folder: ");

        $r11 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r0);

        $r10 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(", dictionary-file: ");

        $r13 = virtualinvoke r2.<org.apache.hadoop.fs.Path: java.lang.String toString()>();

        $r12 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r13);

        $r15 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.String toString()>();

        virtualinvoke r5.<org.apache.hadoop.mapreduce.Job: void setJobName(java.lang.String)>($r15);

        $r14 = newarray (org.apache.hadoop.fs.Path)[1];

        $r14[0] = r1;

        staticinvoke <org.apache.mahout.common.HadoopUtil: void delete(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path[])>($r6, $r14);

        z0 = virtualinvoke r5.<org.apache.hadoop.mapreduce.Job: boolean waitForCompletion(boolean)>(1);

        if z0 != 0 goto label1;

        $r16 = new java.lang.IllegalStateException;

        specialinvoke $r16.<java.lang.IllegalStateException: void <init>(java.lang.String)>("Job failed!");

        throw $r16;

     label1:
        return;
    }

    public static void mergePartialVectors(java.lang.Iterable, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, float, boolean, int) throws java.io.IOException, java.lang.InterruptedException, java.lang.ClassNotFoundException
    {
        java.lang.Iterable r0;
        org.apache.hadoop.fs.Path r1;
        org.apache.hadoop.conf.Configuration r2, $r5;
        float f0;
        boolean z0, z1;
        int i0;
        org.apache.hadoop.mapreduce.Job $r6;
        java.lang.String $r7;
        org.apache.hadoop.fs.Path[] $r8;
        java.lang.IllegalStateException $r9;

        r0 := @parameter0: java.lang.Iterable;

        r1 := @parameter1: org.apache.hadoop.fs.Path;

        r2 := @parameter2: org.apache.hadoop.conf.Configuration;

        f0 := @parameter3: float;

        z0 := @parameter4: boolean;

        i0 := @parameter5: int;

        $r5 = new org.apache.hadoop.conf.Configuration;

        specialinvoke $r5.<org.apache.hadoop.conf.Configuration: void <init>(org.apache.hadoop.conf.Configuration)>(r2);

        virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String)>("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization,org.apache.hadoop.io.serializer.WritableSerialization");

        virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: void setFloat(java.lang.String,float)>("normalization.power", f0);

        virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: void setBoolean(java.lang.String,boolean)>("vector.lognormalize", z0);

        $r6 = new org.apache.hadoop.mapreduce.Job;

        specialinvoke $r6.<org.apache.hadoop.mapreduce.Job: void <init>(org.apache.hadoop.conf.Configuration)>($r5);

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setJobName(java.lang.String)>("PrunerPartialVectorMerger::MergePartialVectors");

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setJarByClass(java.lang.Class)>(class "org/apache/mahout/vectorizer/common/PartialVectorMerger");

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setOutputKeyClass(java.lang.Class)>(class "org/apache/hadoop/io/Text");

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setOutputValueClass(java.lang.Class)>(class "org/apache/mahout/math/VectorWritable");

        $r7 = staticinvoke <org.apache.mahout.vectorizer.HighDFWordsPruner: java.lang.String getCommaSeparatedPaths(java.lang.Iterable)>(r0);

        staticinvoke <org.apache.hadoop.mapreduce.lib.input.FileInputFormat: void setInputPaths(org.apache.hadoop.mapreduce.Job,java.lang.String)>($r6, $r7);

        staticinvoke <org.apache.hadoop.mapreduce.lib.output.FileOutputFormat: void setOutputPath(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path)>($r6, r1);

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setMapperClass(java.lang.Class)>(class "org/apache/hadoop/mapreduce/Mapper");

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setInputFormatClass(java.lang.Class)>(class "org/apache/hadoop/mapreduce/lib/input/SequenceFileInputFormat");

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setReducerClass(java.lang.Class)>(class "org/apache/mahout/vectorizer/pruner/PrunedPartialVectorMergeReducer");

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setOutputFormatClass(java.lang.Class)>(class "org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat");

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setNumReduceTasks(int)>(i0);

        $r8 = newarray (org.apache.hadoop.fs.Path)[1];

        $r8[0] = r1;

        staticinvoke <org.apache.mahout.common.HadoopUtil: void delete(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path[])>($r5, $r8);

        z1 = virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: boolean waitForCompletion(boolean)>(1);

        if z1 != 0 goto label1;

        $r9 = new java.lang.IllegalStateException;

        specialinvoke $r9.<java.lang.IllegalStateException: void <init>(java.lang.String)>("Job failed!");

        throw $r9;

     label1:
        return;
    }

    private static java.lang.String getCommaSeparatedPaths(java.lang.Iterable)
    {
        java.lang.Iterable r0;
        java.util.Iterator r2;
        org.apache.hadoop.fs.Path r3;
        java.lang.StringBuilder $r4, $r7;
        boolean $z0;
        java.lang.String $r5, $r8, r10;
        java.lang.Object $r6;

        r0 := @parameter0: java.lang.Iterable;

        $r4 = new java.lang.StringBuilder;

        specialinvoke $r4.<java.lang.StringBuilder: void <init>(int)>(100);

        r10 = "";

        r2 = interfaceinvoke r0.<java.lang.Iterable: java.util.Iterator iterator()>();

     label1:
        $z0 = interfaceinvoke r2.<java.util.Iterator: boolean hasNext()>();

        if $z0 == 0 goto label2;

        $r6 = interfaceinvoke r2.<java.util.Iterator: java.lang.Object next()>();

        r3 = (org.apache.hadoop.fs.Path) $r6;

        $r7 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(r10);

        $r8 = virtualinvoke r3.<org.apache.hadoop.fs.Path: java.lang.String toString()>();

        virtualinvoke $r7.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r8);

        r10 = ",";

        goto label1;

     label2:
        $r5 = virtualinvoke $r4.<java.lang.StringBuilder: java.lang.String toString()>();

        return $r5;
    }

    public static void <clinit>()
    {
        <org.apache.mahout.vectorizer.HighDFWordsPruner: java.lang.String MIN_DF> = "min.df";

        <org.apache.mahout.vectorizer.HighDFWordsPruner: java.lang.String MAX_DF> = "max.df";

        <org.apache.mahout.vectorizer.HighDFWordsPruner: java.lang.String STD_CALC_DIR> = "stdcalc";

        return;
    }
}
