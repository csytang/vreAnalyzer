public final class org.apache.mahout.vectorizer.DictionaryVectorizer extends org.apache.mahout.common.AbstractJob implements org.apache.mahout.vectorizer.Vectorizer
{
    private static final org.slf4j.Logger log;
    public static final java.lang.String DOCUMENT_VECTOR_OUTPUT_FOLDER;
    public static final java.lang.String MIN_SUPPORT;
    public static final java.lang.String MAX_NGRAMS;
    public static final int DEFAULT_MIN_SUPPORT;
    public static final java.lang.String DICTIONARY_FILE;
    private static final int MAX_CHUNKSIZE;
    private static final int MIN_CHUNKSIZE;
    private static final java.lang.String OUTPUT_FILES_PATTERN;
    private static final int DICTIONARY_BYTE_OVERHEAD;
    private static final java.lang.String VECTOR_OUTPUT_FOLDER;
    private static final java.lang.String DICTIONARY_JOB_FOLDER;

    private void <init>()
    {
        org.apache.mahout.vectorizer.DictionaryVectorizer r0;

        r0 := @this: org.apache.mahout.vectorizer.DictionaryVectorizer;

        specialinvoke r0.<org.apache.mahout.common.AbstractJob: void <init>()>();

        return;
    }

    public void createVectors(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.mahout.vectorizer.VectorizerConfig) throws java.io.IOException, java.lang.ClassNotFoundException, java.lang.InterruptedException
    {
        org.apache.mahout.vectorizer.DictionaryVectorizer r0;
        org.apache.hadoop.fs.Path r1, r2;
        org.apache.mahout.vectorizer.VectorizerConfig r3;
        java.lang.String $r4;
        org.apache.hadoop.conf.Configuration $r5;
        int $i0, $i1, $i2, $i3;
        float $f0, $f1;
        boolean $z0, $z1, $z2;

        r0 := @this: org.apache.mahout.vectorizer.DictionaryVectorizer;

        r1 := @parameter0: org.apache.hadoop.fs.Path;

        r2 := @parameter1: org.apache.hadoop.fs.Path;

        r3 := @parameter2: org.apache.mahout.vectorizer.VectorizerConfig;

        $r4 = virtualinvoke r3.<org.apache.mahout.vectorizer.VectorizerConfig: java.lang.String getTfDirName()>();

        $r5 = virtualinvoke r3.<org.apache.mahout.vectorizer.VectorizerConfig: org.apache.hadoop.conf.Configuration getConf()>();

        $i0 = virtualinvoke r3.<org.apache.mahout.vectorizer.VectorizerConfig: int getMinSupport()>();

        $i1 = virtualinvoke r3.<org.apache.mahout.vectorizer.VectorizerConfig: int getMaxNGramSize()>();

        $f0 = virtualinvoke r3.<org.apache.mahout.vectorizer.VectorizerConfig: float getMinLLRValue()>();

        $f1 = virtualinvoke r3.<org.apache.mahout.vectorizer.VectorizerConfig: float getNormPower()>();

        $z0 = virtualinvoke r3.<org.apache.mahout.vectorizer.VectorizerConfig: boolean isLogNormalize()>();

        $i2 = virtualinvoke r3.<org.apache.mahout.vectorizer.VectorizerConfig: int getNumReducers()>();

        $i3 = virtualinvoke r3.<org.apache.mahout.vectorizer.VectorizerConfig: int getChunkSizeInMegabytes()>();

        $z1 = virtualinvoke r3.<org.apache.mahout.vectorizer.VectorizerConfig: boolean isSequentialAccess()>();

        $z2 = virtualinvoke r3.<org.apache.mahout.vectorizer.VectorizerConfig: boolean isNamedVectors()>();

        staticinvoke <org.apache.mahout.vectorizer.DictionaryVectorizer: void createTermFrequencyVectors(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.conf.Configuration,int,int,float,float,boolean,int,int,boolean,boolean)>(r1, r2, $r4, $r5, $i0, $i1, $f0, $f1, $z0, $i2, $i3, $z1, $z2);

        return;
    }

    public static void createTermFrequencyVectors(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.conf.Configuration, int, int, float, float, boolean, int, int, boolean, boolean) throws java.io.IOException, java.lang.InterruptedException, java.lang.ClassNotFoundException
    {
        org.apache.hadoop.fs.Path r0, r1, $r12, $r14, $r15, $r16, $r21, r26;
        java.lang.String r2, $r22;
        org.apache.hadoop.conf.Configuration r3, $r17;
        int i0, i1, $i4, $i5, $i6, i9, i10, i11;
        float f0, f1;
        boolean z0, z1, z2, $z4, $z5, $z6, $z7;
        int[] r5;
        java.util.ArrayList r6;
        byte $b2, $b3, $b7, $b8;
        java.lang.Object[] $r8, $r10;
        java.lang.Float $r9, $r13;
        org.slf4j.Logger $r11;
        java.lang.StringBuilder $r18, $r19, $r23;
        java.lang.Object $r20;
        double $d0;
        java.util.List r24;
        java.util.Iterator r25;

        r0 := @parameter0: org.apache.hadoop.fs.Path;

        r1 := @parameter1: org.apache.hadoop.fs.Path;

        r2 := @parameter2: java.lang.String;

        r3 := @parameter3: org.apache.hadoop.conf.Configuration;

        i9 := @parameter4: int;

        i0 := @parameter5: int;

        f0 := @parameter6: float;

        f1 := @parameter7: float;

        z0 := @parameter8: boolean;

        i1 := @parameter9: int;

        i10 := @parameter10: int;

        z1 := @parameter11: boolean;

        z2 := @parameter12: boolean;

        $b2 = f1 cmpl -1.0F;

        if $b2 == 0 goto label01;

        $b8 = f1 cmpl 0.0F;

        if $b8 < 0 goto label02;

     label01:
        $z5 = 1;

        goto label03;

     label02:
        $z5 = 0;

     label03:
        $r8 = newarray (java.lang.Object)[1];

        $r9 = staticinvoke <java.lang.Float: java.lang.Float valueOf(float)>(f1);

        $r8[0] = $r9;

        staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,java.lang.Object[])>($z5, "If specified normPower must be nonnegative", $r8);

        $b3 = f1 cmpl -1.0F;

        if $b3 == 0 goto label05;

        $b7 = f1 cmpl 1.0F;

        if $b7 <= 0 goto label04;

        $d0 = (double) f1;

        $z4 = staticinvoke <java.lang.Double: boolean isInfinite(double)>($d0);

        if $z4 == 0 goto label05;

     label04:
        if z0 != 0 goto label06;

     label05:
        $z6 = 1;

        goto label07;

     label06:
        $z6 = 0;

     label07:
        $r10 = newarray (java.lang.Object)[1];

        $r13 = staticinvoke <java.lang.Float: java.lang.Float valueOf(float)>(f1);

        $r10[0] = $r13;

        staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,java.lang.Object[])>($z6, "normPower must be > 1 and not infinite if log normalization is chosen", $r10);

        if i10 >= 100 goto label08;

        i10 = 100;

        goto label09;

     label08:
        if i10 <= 10000 goto label09;

        i10 = 10000;

     label09:
        if i9 >= 0 goto label10;

        i9 = 2;

     label10:
        $r14 = new org.apache.hadoop.fs.Path;

        specialinvoke $r14.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>(r1, "wordcount");

        $r11 = <org.apache.mahout.vectorizer.DictionaryVectorizer: org.slf4j.Logger log>;

        interfaceinvoke $r11.<org.slf4j.Logger: void info(java.lang.String,java.lang.Object,java.lang.Object)>("Creating dictionary from {} and saving at {}", r0, $r14);

        r5 = newarray (int)[1];

        if i0 != 1 goto label11;

        staticinvoke <org.apache.mahout.vectorizer.DictionaryVectorizer: void startWordCounting(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,int)>(r0, $r14, r3, i9);

        r24 = staticinvoke <org.apache.mahout.vectorizer.DictionaryVectorizer: java.util.List createDictionaryChunks(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,int,int[])>($r14, r1, r3, i10, r5);

        goto label12;

     label11:
        staticinvoke <org.apache.mahout.vectorizer.collocations.llr.CollocDriver: void generateAllGrams(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,int,int,float,int)>(r0, $r14, r3, i0, i9, f0, i1);

        $r12 = new org.apache.hadoop.fs.Path;

        $r15 = new org.apache.hadoop.fs.Path;

        specialinvoke $r15.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>(r1, "wordcount");

        specialinvoke $r12.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>($r15, "ngrams");

        r24 = staticinvoke <org.apache.mahout.vectorizer.DictionaryVectorizer: java.util.List createDictionaryChunks(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,int,int[])>($r12, r1, r3, i10, r5);

     label12:
        i11 = 0;

        r6 = staticinvoke <com.google.common.collect.Lists: java.util.ArrayList newArrayList()>();

        r25 = interfaceinvoke r24.<java.util.List: java.util.Iterator iterator()>();

     label13:
        $z7 = interfaceinvoke r25.<java.util.Iterator: boolean hasNext()>();

        if $z7 == 0 goto label14;

        $r20 = interfaceinvoke r25.<java.util.Iterator: java.lang.Object next()>();

        r26 = (org.apache.hadoop.fs.Path) $r20;

        $r21 = new org.apache.hadoop.fs.Path;

        $r18 = new java.lang.StringBuilder;

        specialinvoke $r18.<java.lang.StringBuilder: void <init>()>();

        $r19 = virtualinvoke $r18.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("partial-vectors-");

        $i5 = i11;

        i11 = i11 + 1;

        $r23 = virtualinvoke $r19.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i5);

        $r22 = virtualinvoke $r23.<java.lang.StringBuilder: java.lang.String toString()>();

        specialinvoke $r21.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>(r1, $r22);

        interfaceinvoke r6.<java.util.Collection: boolean add(java.lang.Object)>($r21);

        $i6 = r5[0];

        staticinvoke <org.apache.mahout.vectorizer.DictionaryVectorizer: void makePartialVectors(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,int,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,boolean,boolean,int)>(r0, r3, i0, r26, $r21, $i6, z1, z2, i1);

        goto label13;

     label14:
        $r17 = new org.apache.hadoop.conf.Configuration;

        specialinvoke $r17.<org.apache.hadoop.conf.Configuration: void <init>(org.apache.hadoop.conf.Configuration)>(r3);

        $r16 = new org.apache.hadoop.fs.Path;

        specialinvoke $r16.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>(r1, r2);

        $i4 = r5[0];

        staticinvoke <org.apache.mahout.vectorizer.common.PartialVectorMerger: void mergePartialVectors(java.lang.Iterable,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,float,boolean,int,boolean,boolean,int)>(r6, $r16, $r17, f1, z0, $i4, z1, z2, i1);

        staticinvoke <org.apache.mahout.common.HadoopUtil: void delete(org.apache.hadoop.conf.Configuration,java.lang.Iterable)>($r17, r6);

        return;
    }

    private static java.util.List createDictionaryChunks(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, int, int[]) throws java.io.IOException
    {
        org.apache.hadoop.fs.Path r0, r1, $r15, $r19, $r30;
        org.apache.hadoop.conf.Configuration r2, $r12;
        int i0, i2, $i6, $i7, $i8, $i9, i11, i13;
        int[] r3;
        java.util.ArrayList r4;
        org.apache.hadoop.fs.FileSystem r6;
        long l1, $l3, $l4, $l10, l12;
        java.util.Iterator r8;
        org.apache.hadoop.io.Writable r9;
        org.apache.mahout.common.Pair r10;
        java.net.URI $r13;
        java.lang.StringBuilder $r14, $r16, $r17, $r27, $r28, $r29;
        org.apache.hadoop.io.SequenceFile$Writer $r18, $r31, r35;
        java.lang.String $r20, $r25, $r32;
        boolean $z1;
        org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterable $r21;
        org.apache.mahout.common.iterator.sequencefile.PathType $r22;
        java.lang.Object $r23, $r24;
        byte $b5;
        org.apache.hadoop.io.IntWritable $r26;
        java.lang.Throwable $r33;

        r0 := @parameter0: org.apache.hadoop.fs.Path;

        r1 := @parameter1: org.apache.hadoop.fs.Path;

        r2 := @parameter2: org.apache.hadoop.conf.Configuration;

        i0 := @parameter3: int;

        r3 := @parameter4: int[];

        r4 = staticinvoke <com.google.common.collect.Lists: java.util.ArrayList newArrayList()>();

        $r12 = new org.apache.hadoop.conf.Configuration;

        specialinvoke $r12.<org.apache.hadoop.conf.Configuration: void <init>(org.apache.hadoop.conf.Configuration)>(r2);

        $r13 = virtualinvoke r0.<org.apache.hadoop.fs.Path: java.net.URI toUri()>();

        r6 = staticinvoke <org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FileSystem get(java.net.URI,org.apache.hadoop.conf.Configuration)>($r13, $r12);

        $l4 = (long) i0;

        $l3 = $l4 * 1024L;

        l1 = $l3 * 1024L;

        i11 = 0;

        $r15 = new org.apache.hadoop.fs.Path;

        $r14 = new java.lang.StringBuilder;

        specialinvoke $r14.<java.lang.StringBuilder: void <init>()>();

        $r17 = virtualinvoke $r14.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("dictionary.file-");

        $r16 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(0);

        $r20 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.String toString()>();

        specialinvoke $r15.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>(r1, $r20);

        interfaceinvoke r4.<java.util.List: boolean add(java.lang.Object)>($r15);

        $r18 = new org.apache.hadoop.io.SequenceFile$Writer;

        specialinvoke $r18.<org.apache.hadoop.io.SequenceFile$Writer: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class)>(r6, $r12, $r15, class "org/apache/hadoop/io/Text", class "org/apache/hadoop/io/IntWritable");

        r35 = $r18;

     label1:
        l12 = 0L;

        $r19 = new org.apache.hadoop.fs.Path;

        specialinvoke $r19.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>(r0, "part-*");

        i13 = 0;

        $r21 = new org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterable;

        $r22 = <org.apache.mahout.common.iterator.sequencefile.PathType: org.apache.mahout.common.iterator.sequencefile.PathType GLOB>;

        specialinvoke $r21.<org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterable: void <init>(org.apache.hadoop.fs.Path,org.apache.mahout.common.iterator.sequencefile.PathType,org.apache.hadoop.fs.PathFilter,java.util.Comparator,boolean,org.apache.hadoop.conf.Configuration)>($r19, $r22, null, null, 1, $r12);

        r8 = virtualinvoke $r21.<org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterable: java.util.Iterator iterator()>();

     label2:
        $z1 = interfaceinvoke r8.<java.util.Iterator: boolean hasNext()>();

        if $z1 == 0 goto label4;

        $r24 = interfaceinvoke r8.<java.util.Iterator: java.lang.Object next()>();

        r10 = (org.apache.mahout.common.Pair) $r24;

        $b5 = l12 cmp l1;

        if $b5 <= 0 goto label3;

        staticinvoke <com.google.common.io.Closeables: void close(java.io.Closeable,boolean)>(r35, 0);

        i11 = i11 + 1;

        $r30 = new org.apache.hadoop.fs.Path;

        $r29 = new java.lang.StringBuilder;

        specialinvoke $r29.<java.lang.StringBuilder: void <init>()>();

        $r28 = virtualinvoke $r29.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("dictionary.file-");

        $r27 = virtualinvoke $r28.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(i11);

        $r32 = virtualinvoke $r27.<java.lang.StringBuilder: java.lang.String toString()>();

        specialinvoke $r30.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>(r1, $r32);

        interfaceinvoke r4.<java.util.List: boolean add(java.lang.Object)>($r30);

        $r31 = new org.apache.hadoop.io.SequenceFile$Writer;

        specialinvoke $r31.<org.apache.hadoop.io.SequenceFile$Writer: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class)>(r6, $r12, $r30, class "org/apache/hadoop/io/Text", class "org/apache/hadoop/io/IntWritable");

        r35 = $r31;

        l12 = 0L;

     label3:
        $r23 = virtualinvoke r10.<org.apache.mahout.common.Pair: java.lang.Object getFirst()>();

        r9 = (org.apache.hadoop.io.Writable) $r23;

        $r25 = virtualinvoke r9.<java.lang.Object: java.lang.String toString()>();

        $i8 = virtualinvoke $r25.<java.lang.String: int length()>();

        $i6 = $i8 * 2;

        $i7 = 4 + $i6;

        i2 = $i7 + 4;

        $l10 = (long) i2;

        l12 = l12 + $l10;

        $r26 = new org.apache.hadoop.io.IntWritable;

        $i9 = i13;

        i13 = i13 + 1;

        specialinvoke $r26.<org.apache.hadoop.io.IntWritable: void <init>(int)>($i9);

        virtualinvoke r35.<org.apache.hadoop.io.SequenceFile$Writer: void append(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)>(r9, $r26);

        goto label2;

     label4:
        r3[0] = i13;

     label5:
        staticinvoke <com.google.common.io.Closeables: void close(java.io.Closeable,boolean)>(r35, 0);

        goto label8;

     label6:
        $r33 := @caughtexception;

     label7:
        staticinvoke <com.google.common.io.Closeables: void close(java.io.Closeable,boolean)>(r35, 0);

        throw $r33;

     label8:
        return r4;

        catch java.lang.Throwable from label1 to label5 with label6;
        catch java.lang.Throwable from label6 to label7 with label6;
    }

    private static void makePartialVectors(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, int, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, int, boolean, boolean, int) throws java.io.IOException, java.lang.InterruptedException, java.lang.ClassNotFoundException
    {
        org.apache.hadoop.fs.Path r0, r2, r3;
        org.apache.hadoop.conf.Configuration r1, $r6;
        int i0, i1, i2;
        boolean z0, z1, z2;
        java.net.URI $r7;
        org.apache.hadoop.mapreduce.Job $r8;
        java.lang.StringBuilder $r9, $r10, $r11, $r12, $r13;
        org.apache.hadoop.fs.Path[] $r14, $r16;
        java.lang.String $r15;
        java.lang.IllegalStateException $r17;

        r0 := @parameter0: org.apache.hadoop.fs.Path;

        r1 := @parameter1: org.apache.hadoop.conf.Configuration;

        i0 := @parameter2: int;

        r2 := @parameter3: org.apache.hadoop.fs.Path;

        r3 := @parameter4: org.apache.hadoop.fs.Path;

        i1 := @parameter5: int;

        z0 := @parameter6: boolean;

        z1 := @parameter7: boolean;

        i2 := @parameter8: int;

        $r6 = new org.apache.hadoop.conf.Configuration;

        specialinvoke $r6.<org.apache.hadoop.conf.Configuration: void <init>(org.apache.hadoop.conf.Configuration)>(r1);

        virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String)>("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization,org.apache.hadoop.io.serializer.WritableSerialization");

        virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: void setInt(java.lang.String,int)>("vector.dimension", i1);

        virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: void setBoolean(java.lang.String,boolean)>("vector.sequentialAccess", z0);

        virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: void setBoolean(java.lang.String,boolean)>("vector.named", z1);

        virtualinvoke $r6.<org.apache.hadoop.conf.Configuration: void setInt(java.lang.String,int)>("max.ngrams", i0);

        $r7 = virtualinvoke r2.<org.apache.hadoop.fs.Path: java.net.URI toUri()>();

        staticinvoke <org.apache.hadoop.filecache.DistributedCache: void addCacheFile(java.net.URI,org.apache.hadoop.conf.Configuration)>($r7, $r6);

        $r8 = new org.apache.hadoop.mapreduce.Job;

        specialinvoke $r8.<org.apache.hadoop.mapreduce.Job: void <init>(org.apache.hadoop.conf.Configuration)>($r6);

        $r9 = new java.lang.StringBuilder;

        specialinvoke $r9.<java.lang.StringBuilder: void <init>()>();

        $r11 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("DictionaryVectorizer::MakePartialVectors: input-folder: ");

        $r10 = virtualinvoke $r11.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r0);

        $r13 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(", dictionary-file: ");

        $r12 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r2);

        $r15 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.String toString()>();

        virtualinvoke $r8.<org.apache.hadoop.mapreduce.Job: void setJobName(java.lang.String)>($r15);

        virtualinvoke $r8.<org.apache.hadoop.mapreduce.Job: void setJarByClass(java.lang.Class)>(class "org/apache/mahout/vectorizer/DictionaryVectorizer");

        virtualinvoke $r8.<org.apache.hadoop.mapreduce.Job: void setMapOutputKeyClass(java.lang.Class)>(class "org/apache/hadoop/io/Text");

        virtualinvoke $r8.<org.apache.hadoop.mapreduce.Job: void setMapOutputValueClass(java.lang.Class)>(class "org/apache/mahout/common/StringTuple");

        virtualinvoke $r8.<org.apache.hadoop.mapreduce.Job: void setOutputKeyClass(java.lang.Class)>(class "org/apache/hadoop/io/Text");

        virtualinvoke $r8.<org.apache.hadoop.mapreduce.Job: void setOutputValueClass(java.lang.Class)>(class "org/apache/mahout/math/VectorWritable");

        $r14 = newarray (org.apache.hadoop.fs.Path)[1];

        $r14[0] = r0;

        staticinvoke <org.apache.hadoop.mapreduce.lib.input.FileInputFormat: void setInputPaths(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path[])>($r8, $r14);

        staticinvoke <org.apache.hadoop.mapreduce.lib.output.FileOutputFormat: void setOutputPath(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path)>($r8, r3);

        virtualinvoke $r8.<org.apache.hadoop.mapreduce.Job: void setMapperClass(java.lang.Class)>(class "org/apache/hadoop/mapreduce/Mapper");

        virtualinvoke $r8.<org.apache.hadoop.mapreduce.Job: void setInputFormatClass(java.lang.Class)>(class "org/apache/hadoop/mapreduce/lib/input/SequenceFileInputFormat");

        virtualinvoke $r8.<org.apache.hadoop.mapreduce.Job: void setReducerClass(java.lang.Class)>(class "org/apache/mahout/vectorizer/term/TFPartialVectorReducer");

        virtualinvoke $r8.<org.apache.hadoop.mapreduce.Job: void setOutputFormatClass(java.lang.Class)>(class "org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat");

        virtualinvoke $r8.<org.apache.hadoop.mapreduce.Job: void setNumReduceTasks(int)>(i2);

        $r16 = newarray (org.apache.hadoop.fs.Path)[1];

        $r16[0] = r3;

        staticinvoke <org.apache.mahout.common.HadoopUtil: void delete(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path[])>($r6, $r16);

        z2 = virtualinvoke $r8.<org.apache.hadoop.mapreduce.Job: boolean waitForCompletion(boolean)>(1);

        if z2 != 0 goto label1;

        $r17 = new java.lang.IllegalStateException;

        specialinvoke $r17.<java.lang.IllegalStateException: void <init>(java.lang.String)>("Job failed!");

        throw $r17;

     label1:
        return;
    }

    private static void startWordCounting(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, int) throws java.io.IOException, java.lang.InterruptedException, java.lang.ClassNotFoundException
    {
        org.apache.hadoop.fs.Path r0, r1;
        org.apache.hadoop.conf.Configuration r2, $r5;
        int i0;
        boolean z0;
        org.apache.hadoop.mapreduce.Job $r6;
        java.lang.StringBuilder $r7, $r8, $r9;
        java.lang.String $r10;
        org.apache.hadoop.fs.Path[] $r11, $r12;
        java.lang.IllegalStateException $r13;

        r0 := @parameter0: org.apache.hadoop.fs.Path;

        r1 := @parameter1: org.apache.hadoop.fs.Path;

        r2 := @parameter2: org.apache.hadoop.conf.Configuration;

        i0 := @parameter3: int;

        $r5 = new org.apache.hadoop.conf.Configuration;

        specialinvoke $r5.<org.apache.hadoop.conf.Configuration: void <init>(org.apache.hadoop.conf.Configuration)>(r2);

        virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String)>("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization,org.apache.hadoop.io.serializer.WritableSerialization");

        virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: void setInt(java.lang.String,int)>("min.support", i0);

        $r6 = new org.apache.hadoop.mapreduce.Job;

        specialinvoke $r6.<org.apache.hadoop.mapreduce.Job: void <init>(org.apache.hadoop.conf.Configuration)>($r5);

        $r7 = new java.lang.StringBuilder;

        specialinvoke $r7.<java.lang.StringBuilder: void <init>()>();

        $r8 = virtualinvoke $r7.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("DictionaryVectorizer::WordCount: input-folder: ");

        $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r0);

        $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.String toString()>();

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setJobName(java.lang.String)>($r10);

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setJarByClass(java.lang.Class)>(class "org/apache/mahout/vectorizer/DictionaryVectorizer");

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setOutputKeyClass(java.lang.Class)>(class "org/apache/hadoop/io/Text");

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setOutputValueClass(java.lang.Class)>(class "org/apache/hadoop/io/LongWritable");

        $r11 = newarray (org.apache.hadoop.fs.Path)[1];

        $r11[0] = r0;

        staticinvoke <org.apache.hadoop.mapreduce.lib.input.FileInputFormat: void setInputPaths(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path[])>($r6, $r11);

        staticinvoke <org.apache.hadoop.mapreduce.lib.output.FileOutputFormat: void setOutputPath(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path)>($r6, r1);

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setMapperClass(java.lang.Class)>(class "org/apache/mahout/vectorizer/term/TermCountMapper");

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setInputFormatClass(java.lang.Class)>(class "org/apache/hadoop/mapreduce/lib/input/SequenceFileInputFormat");

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setCombinerClass(java.lang.Class)>(class "org/apache/mahout/vectorizer/term/TermCountCombiner");

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setReducerClass(java.lang.Class)>(class "org/apache/mahout/vectorizer/term/TermCountReducer");

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setOutputFormatClass(java.lang.Class)>(class "org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat");

        $r12 = newarray (org.apache.hadoop.fs.Path)[1];

        $r12[0] = r1;

        staticinvoke <org.apache.mahout.common.HadoopUtil: void delete(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path[])>($r5, $r12);

        z0 = virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: boolean waitForCompletion(boolean)>(1);

        if z0 != 0 goto label1;

        $r13 = new java.lang.IllegalStateException;

        specialinvoke $r13.<java.lang.IllegalStateException: void <init>(java.lang.String)>("Job failed!");

        throw $r13;

     label1:
        return;
    }

    public int run(java.lang.String[]) throws java.lang.Exception
    {
        org.apache.mahout.vectorizer.DictionaryVectorizer r0;
        java.lang.String[] r1;
        java.lang.String r2;
        int i0, i1, i2, i3;
        float f0, f1;
        boolean z0, z1, z2;
        org.apache.commons.cli2.builder.DefaultOptionBuilder $r3, $r6;
        org.apache.commons.cli2.option.DefaultOption $r4, $r8;
        java.util.Map $r9;
        org.apache.hadoop.fs.Path $r10, $r12;
        org.apache.hadoop.conf.Configuration $r11;

        r0 := @this: org.apache.mahout.vectorizer.DictionaryVectorizer;

        r1 := @parameter0: java.lang.String[];

        virtualinvoke r0.<org.apache.mahout.vectorizer.DictionaryVectorizer: void addInputOption()>();

        virtualinvoke r0.<org.apache.mahout.vectorizer.DictionaryVectorizer: void addOutputOption()>();

        virtualinvoke r0.<org.apache.mahout.vectorizer.DictionaryVectorizer: void addOption(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>("tfDirName", "tf", "The folder to store the TF calculations", "tfDirName");

        virtualinvoke r0.<org.apache.mahout.vectorizer.DictionaryVectorizer: void addOption(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>("minSupport", "s", "(Optional) Minimum Support. Default Value: 2", "2");

        virtualinvoke r0.<org.apache.mahout.vectorizer.DictionaryVectorizer: void addOption(java.lang.String,java.lang.String,java.lang.String)>("maxNGramSize", "ng", "(Optional) The maximum size of ngrams to create (2 = bigrams, 3 = trigrams, etc) Default Value:1");

        virtualinvoke r0.<org.apache.mahout.vectorizer.DictionaryVectorizer: void addOption(java.lang.String,java.lang.String,java.lang.String)>("minLLR", "ml", "(Optional)The minimum Log Likelihood Ratio(Float)  Default is 1.0");

        virtualinvoke r0.<org.apache.mahout.vectorizer.DictionaryVectorizer: void addOption(java.lang.String,java.lang.String,java.lang.String)>("norm", "n", "The norm to use, expressed as either a float or \"INF\" if you want to use the Infinite norm.  Must be greater or equal to 0.  The default is not to normalize");

        virtualinvoke r0.<org.apache.mahout.vectorizer.DictionaryVectorizer: void addOption(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>("logNormalize", "lnorm", "(Optional) Whether output vectors should be logNormalize. If set true else false", "false");

        $r3 = staticinvoke <org.apache.mahout.common.commandline.DefaultOptionCreator: org.apache.commons.cli2.builder.DefaultOptionBuilder numReducersOption()>();

        $r4 = virtualinvoke $r3.<org.apache.commons.cli2.builder.DefaultOptionBuilder: org.apache.commons.cli2.option.DefaultOption create()>();

        virtualinvoke r0.<org.apache.mahout.vectorizer.DictionaryVectorizer: org.apache.commons.cli2.Option addOption(org.apache.commons.cli2.Option)>($r4);

        virtualinvoke r0.<org.apache.mahout.vectorizer.DictionaryVectorizer: void addOption(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>("chunkSize", "chunk", "The chunkSize in MegaBytes. 100-10000 MB", "100");

        $r6 = staticinvoke <org.apache.mahout.common.commandline.DefaultOptionCreator: org.apache.commons.cli2.builder.DefaultOptionBuilder methodOption()>();

        $r8 = virtualinvoke $r6.<org.apache.commons.cli2.builder.DefaultOptionBuilder: org.apache.commons.cli2.option.DefaultOption create()>();

        virtualinvoke r0.<org.apache.mahout.vectorizer.DictionaryVectorizer: org.apache.commons.cli2.Option addOption(org.apache.commons.cli2.Option)>($r8);

        virtualinvoke r0.<org.apache.mahout.vectorizer.DictionaryVectorizer: void addOption(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>("namedVector", "nv", "(Optional) Whether output vectors should be NamedVectors. If set true else false", "false");

        $r9 = virtualinvoke r0.<org.apache.mahout.vectorizer.DictionaryVectorizer: java.util.Map parseArguments(java.lang.String[])>(r1);

        if $r9 != null goto label1;

        return -1;

     label1:
        r2 = virtualinvoke r0.<org.apache.mahout.vectorizer.DictionaryVectorizer: java.lang.String getOption(java.lang.String,java.lang.String)>("tfDirName", "tfDir");

        i0 = virtualinvoke r0.<org.apache.mahout.vectorizer.DictionaryVectorizer: int getInt(java.lang.String,int)>("minSupport", 2);

        i1 = virtualinvoke r0.<org.apache.mahout.vectorizer.DictionaryVectorizer: int getInt(java.lang.String,int)>("maxNGramSize", 1);

        f0 = virtualinvoke r0.<org.apache.mahout.vectorizer.DictionaryVectorizer: float getFloat(java.lang.String,float)>("minLLR", 1.0F);

        f1 = virtualinvoke r0.<org.apache.mahout.vectorizer.DictionaryVectorizer: float getFloat(java.lang.String,float)>("norm", -1.0F);

        z0 = virtualinvoke r0.<org.apache.mahout.vectorizer.DictionaryVectorizer: boolean hasOption(java.lang.String)>("logNormalize");

        i2 = virtualinvoke r0.<org.apache.mahout.vectorizer.DictionaryVectorizer: int getInt(java.lang.String)>("maxRed");

        i3 = virtualinvoke r0.<org.apache.mahout.vectorizer.DictionaryVectorizer: int getInt(java.lang.String,int)>("chunkSize", 100);

        z1 = virtualinvoke r0.<org.apache.mahout.vectorizer.DictionaryVectorizer: boolean hasOption(java.lang.String)>("sequential");

        z2 = virtualinvoke r0.<org.apache.mahout.vectorizer.DictionaryVectorizer: boolean hasOption(java.lang.String)>("namedVectors");

        $r10 = virtualinvoke r0.<org.apache.mahout.vectorizer.DictionaryVectorizer: org.apache.hadoop.fs.Path getInputPath()>();

        $r12 = virtualinvoke r0.<org.apache.mahout.vectorizer.DictionaryVectorizer: org.apache.hadoop.fs.Path getOutputPath()>();

        $r11 = virtualinvoke r0.<org.apache.mahout.vectorizer.DictionaryVectorizer: org.apache.hadoop.conf.Configuration getConf()>();

        staticinvoke <org.apache.mahout.vectorizer.DictionaryVectorizer: void createTermFrequencyVectors(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.conf.Configuration,int,int,float,float,boolean,int,int,boolean,boolean)>($r10, $r12, r2, $r11, i0, i1, f0, f1, z0, i2, i3, z1, z2);

        return 0;
    }

    public static void main(java.lang.String[]) throws java.lang.Exception
    {
        java.lang.String[] r0;
        org.apache.mahout.vectorizer.DictionaryVectorizer $r1;

        r0 := @parameter0: java.lang.String[];

        $r1 = new org.apache.mahout.vectorizer.DictionaryVectorizer;

        specialinvoke $r1.<org.apache.mahout.vectorizer.DictionaryVectorizer: void <init>()>();

        staticinvoke <org.apache.hadoop.util.ToolRunner: int run(org.apache.hadoop.util.Tool,java.lang.String[])>($r1, r0);

        return;
    }

    static void <clinit>()
    {
        org.slf4j.Logger $r0;

        <org.apache.mahout.vectorizer.DictionaryVectorizer: java.lang.String DICTIONARY_JOB_FOLDER> = "wordcount";

        <org.apache.mahout.vectorizer.DictionaryVectorizer: java.lang.String VECTOR_OUTPUT_FOLDER> = "partial-vectors-";

        <org.apache.mahout.vectorizer.DictionaryVectorizer: int DICTIONARY_BYTE_OVERHEAD> = 4;

        <org.apache.mahout.vectorizer.DictionaryVectorizer: java.lang.String OUTPUT_FILES_PATTERN> = "part-*";

        <org.apache.mahout.vectorizer.DictionaryVectorizer: int MIN_CHUNKSIZE> = 100;

        <org.apache.mahout.vectorizer.DictionaryVectorizer: int MAX_CHUNKSIZE> = 10000;

        <org.apache.mahout.vectorizer.DictionaryVectorizer: java.lang.String DICTIONARY_FILE> = "dictionary.file-";

        <org.apache.mahout.vectorizer.DictionaryVectorizer: int DEFAULT_MIN_SUPPORT> = 2;

        <org.apache.mahout.vectorizer.DictionaryVectorizer: java.lang.String MAX_NGRAMS> = "max.ngrams";

        <org.apache.mahout.vectorizer.DictionaryVectorizer: java.lang.String MIN_SUPPORT> = "min.support";

        <org.apache.mahout.vectorizer.DictionaryVectorizer: java.lang.String DOCUMENT_VECTOR_OUTPUT_FOLDER> = "tf-vectors";

        $r0 = staticinvoke <org.slf4j.LoggerFactory: org.slf4j.Logger getLogger(java.lang.Class)>(class "org/apache/mahout/vectorizer/DictionaryVectorizer");

        <org.apache.mahout.vectorizer.DictionaryVectorizer: org.slf4j.Logger log> = $r0;

        return;
    }
}
