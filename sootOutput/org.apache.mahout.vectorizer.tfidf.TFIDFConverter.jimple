public final class org.apache.mahout.vectorizer.tfidf.TFIDFConverter extends java.lang.Object
{
    public static final java.lang.String VECTOR_COUNT;
    public static final java.lang.String FEATURE_COUNT;
    public static final java.lang.String MIN_DF;
    public static final java.lang.String MAX_DF;
    private static final java.lang.String DOCUMENT_VECTOR_OUTPUT_FOLDER;
    public static final java.lang.String FREQUENCY_FILE;
    private static final int MAX_CHUNKSIZE;
    private static final int MIN_CHUNKSIZE;
    private static final java.lang.String OUTPUT_FILES_PATTERN;
    private static final int SEQUENCEFILE_BYTE_OVERHEAD;
    private static final java.lang.String VECTOR_OUTPUT_FOLDER;
    public static final java.lang.String WORDCOUNT_OUTPUT_FOLDER;

    private void <init>()
    {
        org.apache.mahout.vectorizer.tfidf.TFIDFConverter r0;

        r0 := @this: org.apache.mahout.vectorizer.tfidf.TFIDFConverter;

        specialinvoke r0.<java.lang.Object: void <init>()>();

        return;
    }

    public static void processTfIdf(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.mahout.common.Pair, int, long, float, boolean, boolean, boolean, int) throws java.io.IOException, java.lang.InterruptedException, java.lang.ClassNotFoundException
    {
        org.apache.hadoop.fs.Path r0, r1, $r13, $r20, r30;
        org.apache.hadoop.conf.Configuration r2, $r12;
        org.apache.mahout.common.Pair r3;
        int i0, i2, $i5, $i6, i9;
        long l1;
        float f0;
        boolean z0, z1, z2, $z4, $z5, $z6, $z7;
        java.util.ArrayList r4;
        java.util.List r5;
        java.lang.Object[] $r7, $r9;
        byte $b3, $b4, $b7, $b8;
        java.lang.Float $r8, $r10;
        java.lang.Object $r11, $r15, $r19, $r23, $r26;
        java.lang.Long $r14, $r24, $r27;
        java.lang.Long[] $r16, $r25, $r28;
        java.lang.StringBuilder $r17, $r18, $r22;
        java.lang.String $r21;
        double $d0;
        java.util.Iterator r29;

        r0 := @parameter0: org.apache.hadoop.fs.Path;

        r1 := @parameter1: org.apache.hadoop.fs.Path;

        r2 := @parameter2: org.apache.hadoop.conf.Configuration;

        r3 := @parameter3: org.apache.mahout.common.Pair;

        i0 := @parameter4: int;

        l1 := @parameter5: long;

        f0 := @parameter6: float;

        z0 := @parameter7: boolean;

        z1 := @parameter8: boolean;

        z2 := @parameter9: boolean;

        i2 := @parameter10: int;

        $b3 = f0 cmpl -1.0F;

        if $b3 == 0 goto label1;

        $b8 = f0 cmpl 0.0F;

        if $b8 < 0 goto label2;

     label1:
        $z5 = 1;

        goto label3;

     label2:
        $z5 = 0;

     label3:
        $r7 = newarray (java.lang.Object)[1];

        $r8 = staticinvoke <java.lang.Float: java.lang.Float valueOf(float)>(f0);

        $r7[0] = $r8;

        staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,java.lang.Object[])>($z5, "If specified normPower must be nonnegative", $r7);

        $b4 = f0 cmpl -1.0F;

        if $b4 == 0 goto label5;

        $b7 = f0 cmpl 1.0F;

        if $b7 <= 0 goto label4;

        $d0 = (double) f0;

        $z4 = staticinvoke <java.lang.Double: boolean isInfinite(double)>($d0);

        if $z4 == 0 goto label5;

     label4:
        if z0 != 0 goto label6;

     label5:
        $z6 = 1;

        goto label7;

     label6:
        $z6 = 0;

     label7:
        $r9 = newarray (java.lang.Object)[1];

        $r10 = staticinvoke <java.lang.Float: java.lang.Float valueOf(float)>(f0);

        $r9[0] = $r10;

        staticinvoke <com.google.common.base.Preconditions: void checkArgument(boolean,java.lang.String,java.lang.Object[])>($z6, "normPower must be > 1 and not infinite if log normalization is chosen", $r9);

        i9 = 0;

        r4 = staticinvoke <com.google.common.collect.Lists: java.util.ArrayList newArrayList()>();

        $r11 = virtualinvoke r3.<org.apache.mahout.common.Pair: java.lang.Object getSecond()>();

        r5 = (java.util.List) $r11;

        r29 = interfaceinvoke r5.<java.util.List: java.util.Iterator iterator()>();

     label8:
        $z7 = interfaceinvoke r29.<java.util.Iterator: boolean hasNext()>();

        if $z7 == 0 goto label9;

        $r19 = interfaceinvoke r29.<java.util.Iterator: java.lang.Object next()>();

        r30 = (org.apache.hadoop.fs.Path) $r19;

        $r20 = new org.apache.hadoop.fs.Path;

        $r17 = new java.lang.StringBuilder;

        specialinvoke $r17.<java.lang.StringBuilder: void <init>()>();

        $r18 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("partial-vectors-");

        $i6 = i9;

        i9 = i9 + 1;

        $r22 = virtualinvoke $r18.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>($i6);

        $r21 = virtualinvoke $r22.<java.lang.StringBuilder: java.lang.String toString()>();

        specialinvoke $r20.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>(r1, $r21);

        interfaceinvoke r4.<java.util.List: boolean add(java.lang.Object)>($r20);

        $r26 = virtualinvoke r3.<org.apache.mahout.common.Pair: java.lang.Object getFirst()>();

        $r25 = (java.lang.Long[]) $r26;

        $r24 = $r25[0];

        $r23 = virtualinvoke r3.<org.apache.mahout.common.Pair: java.lang.Object getFirst()>();

        $r28 = (java.lang.Long[]) $r23;

        $r27 = $r28[1];

        staticinvoke <org.apache.mahout.vectorizer.tfidf.TFIDFConverter: void makePartialVectors(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.lang.Long,java.lang.Long,int,long,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean)>(r0, r2, $r24, $r27, i0, l1, r30, $r20, z1, z2);

        goto label8;

     label9:
        $r12 = new org.apache.hadoop.conf.Configuration;

        specialinvoke $r12.<org.apache.hadoop.conf.Configuration: void <init>(org.apache.hadoop.conf.Configuration)>(r2);

        $r13 = new org.apache.hadoop.fs.Path;

        specialinvoke $r13.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>(r1, "tfidf-vectors");

        $r15 = virtualinvoke r3.<org.apache.mahout.common.Pair: java.lang.Object getFirst()>();

        $r16 = (java.lang.Long[]) $r15;

        $r14 = $r16[0];

        $i5 = virtualinvoke $r14.<java.lang.Long: int intValue()>();

        staticinvoke <org.apache.mahout.vectorizer.common.PartialVectorMerger: void mergePartialVectors(java.lang.Iterable,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,float,boolean,int,boolean,boolean,int)>(r4, $r13, r2, f0, z0, $i5, z1, z2, i2);

        staticinvoke <org.apache.mahout.common.HadoopUtil: void delete(org.apache.hadoop.conf.Configuration,java.lang.Iterable)>($r12, r4);

        return;
    }

    public static org.apache.mahout.common.Pair calculateDF(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, int) throws java.io.IOException, java.lang.InterruptedException, java.lang.ClassNotFoundException
    {
        org.apache.hadoop.fs.Path r0, r1, $r4;
        org.apache.hadoop.conf.Configuration r2;
        int i0;
        org.apache.mahout.common.Pair $r5;

        r0 := @parameter0: org.apache.hadoop.fs.Path;

        r1 := @parameter1: org.apache.hadoop.fs.Path;

        r2 := @parameter2: org.apache.hadoop.conf.Configuration;

        i0 := @parameter3: int;

        if i0 >= 100 goto label1;

        i0 = 100;

        goto label2;

     label1:
        if i0 <= 10000 goto label2;

        i0 = 10000;

     label2:
        $r4 = new org.apache.hadoop.fs.Path;

        specialinvoke $r4.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>(r1, "df-count");

        staticinvoke <org.apache.mahout.vectorizer.tfidf.TFIDFConverter: void startDFCounting(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>(r0, $r4, r2);

        $r5 = staticinvoke <org.apache.mahout.vectorizer.tfidf.TFIDFConverter: org.apache.mahout.common.Pair createDictionaryChunks(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,int)>($r4, r1, r2, i0);

        return $r5;
    }

    private static org.apache.mahout.common.Pair createDictionaryChunks(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, int) throws java.io.IOException
    {
        org.apache.hadoop.fs.Path r0, r1, $r12, $r17, $r28;
        org.apache.hadoop.conf.Configuration r2, $r11;
        int i0, $i7, $i8, i9, $i13;
        java.util.ArrayList r3;
        org.apache.hadoop.fs.FileSystem r5;
        long l1, $l3, $l4, $l5, l10, l11, l12, $l14, l15;
        org.apache.hadoop.io.LongWritable r7;
        org.apache.hadoop.io.IntWritable r8;
        java.net.URI $r10;
        java.lang.StringBuilder $r13, $r15, $r16, $r30, $r31, $r32;
        java.lang.String $r14, $r33;
        org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterable $r18;
        org.apache.hadoop.io.SequenceFile$Writer $r19, $r29, r36;
        boolean $z1;
        java.lang.Long[] $r20;
        org.apache.mahout.common.iterator.sequencefile.PathType $r21;
        java.lang.Long $r22, $r23;
        byte $b6;
        java.lang.Object $r24, $r26, $r27;
        org.apache.mahout.common.Pair $r25, r38;
        java.lang.Throwable $r34;
        java.util.Iterator r37;

        r0 := @parameter0: org.apache.hadoop.fs.Path;

        r1 := @parameter1: org.apache.hadoop.fs.Path;

        r2 := @parameter2: org.apache.hadoop.conf.Configuration;

        i0 := @parameter3: int;

        r3 = staticinvoke <com.google.common.collect.Lists: java.util.ArrayList newArrayList()>();

        $r11 = new org.apache.hadoop.conf.Configuration;

        specialinvoke $r11.<org.apache.hadoop.conf.Configuration: void <init>(org.apache.hadoop.conf.Configuration)>(r2);

        $r10 = virtualinvoke r0.<org.apache.hadoop.fs.Path: java.net.URI toUri()>();

        r5 = staticinvoke <org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FileSystem get(java.net.URI,org.apache.hadoop.conf.Configuration)>($r10, $r11);

        $l3 = (long) i0;

        $l4 = $l3 * 1024L;

        l1 = $l4 * 1024L;

        i9 = 0;

        $r12 = new org.apache.hadoop.fs.Path;

        $r15 = new java.lang.StringBuilder;

        specialinvoke $r15.<java.lang.StringBuilder: void <init>()>();

        $r16 = virtualinvoke $r15.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("frequency.file-");

        $r13 = virtualinvoke $r16.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(0);

        $r14 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.String toString()>();

        specialinvoke $r12.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>(r1, $r14);

        interfaceinvoke r3.<java.util.List: boolean add(java.lang.Object)>($r12);

        $r19 = new org.apache.hadoop.io.SequenceFile$Writer;

        specialinvoke $r19.<org.apache.hadoop.io.SequenceFile$Writer: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class)>(r5, $r11, $r12, class "org/apache/hadoop/io/IntWritable", class "org/apache/hadoop/io/LongWritable");

        r36 = $r19;

     label1:
        l10 = 0L;

        l11 = 0L;

        l12 = 9223372036854775807L;

        $r17 = new org.apache.hadoop.fs.Path;

        specialinvoke $r17.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>(r0, "part-*");

        $r18 = new org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterable;

        $r21 = <org.apache.mahout.common.iterator.sequencefile.PathType: org.apache.mahout.common.iterator.sequencefile.PathType GLOB>;

        specialinvoke $r18.<org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterable: void <init>(org.apache.hadoop.fs.Path,org.apache.mahout.common.iterator.sequencefile.PathType,org.apache.hadoop.fs.PathFilter,java.util.Comparator,boolean,org.apache.hadoop.conf.Configuration)>($r17, $r21, null, null, 1, $r11);

        r37 = virtualinvoke $r18.<org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterable: java.util.Iterator iterator()>();

     label2:
        $z1 = interfaceinvoke r37.<java.util.Iterator: boolean hasNext()>();

        if $z1 == 0 goto label6;

        $r24 = interfaceinvoke r37.<java.util.Iterator: java.lang.Object next()>();

        r38 = (org.apache.mahout.common.Pair) $r24;

        $b6 = l10 cmp l1;

        if $b6 <= 0 goto label3;

        staticinvoke <com.google.common.io.Closeables: void close(java.io.Closeable,boolean)>(r36, 0);

        i9 = i9 + 1;

        $r28 = new org.apache.hadoop.fs.Path;

        $r30 = new java.lang.StringBuilder;

        specialinvoke $r30.<java.lang.StringBuilder: void <init>()>();

        $r31 = virtualinvoke $r30.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("frequency.file-");

        $r32 = virtualinvoke $r31.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(i9);

        $r33 = virtualinvoke $r32.<java.lang.StringBuilder: java.lang.String toString()>();

        specialinvoke $r28.<org.apache.hadoop.fs.Path: void <init>(org.apache.hadoop.fs.Path,java.lang.String)>(r1, $r33);

        interfaceinvoke r3.<java.util.List: boolean add(java.lang.Object)>($r28);

        $r29 = new org.apache.hadoop.io.SequenceFile$Writer;

        specialinvoke $r29.<org.apache.hadoop.io.SequenceFile$Writer: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class)>(r5, $r11, $r28, class "org/apache/hadoop/io/IntWritable", class "org/apache/hadoop/io/LongWritable");

        r36 = $r29;

        l10 = 0L;

     label3:
        $l5 = (long) 57;

        l10 = l10 + $l5;

        $r27 = virtualinvoke r38.<org.apache.mahout.common.Pair: java.lang.Object getFirst()>();

        r8 = (org.apache.hadoop.io.IntWritable) $r27;

        $r26 = virtualinvoke r38.<org.apache.mahout.common.Pair: java.lang.Object getSecond()>();

        r7 = (org.apache.hadoop.io.LongWritable) $r26;

        $i8 = virtualinvoke r8.<org.apache.hadoop.io.IntWritable: int get()>();

        if $i8 < 0 goto label4;

        virtualinvoke r36.<org.apache.hadoop.io.SequenceFile$Writer: void append(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)>(r8, r7);

        goto label5;

     label4:
        $i7 = virtualinvoke r8.<org.apache.hadoop.io.IntWritable: int get()>();

        if $i7 != -1 goto label5;

        l12 = virtualinvoke r7.<org.apache.hadoop.io.LongWritable: long get()>();

     label5:
        $i13 = virtualinvoke r8.<org.apache.hadoop.io.IntWritable: int get()>();

        $l14 = (long) $i13;

        l11 = staticinvoke <java.lang.Math: long max(long,long)>($l14, l11);

        goto label2;

     label6:
        l15 = l11 + 1L;

        $r20 = newarray (java.lang.Long)[2];

        $r23 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l15);

        $r20[0] = $r23;

        $r22 = staticinvoke <java.lang.Long: java.lang.Long valueOf(long)>(l12);

        $r20[1] = $r22;

        $r25 = new org.apache.mahout.common.Pair;

        specialinvoke $r25.<org.apache.mahout.common.Pair: void <init>(java.lang.Object,java.lang.Object)>($r20, r3);

     label7:
        staticinvoke <com.google.common.io.Closeables: void close(java.io.Closeable,boolean)>(r36, 0);

        return $r25;

     label8:
        $r34 := @caughtexception;

     label9:
        staticinvoke <com.google.common.io.Closeables: void close(java.io.Closeable,boolean)>(r36, 0);

        throw $r34;

        catch java.lang.Throwable from label1 to label7 with label8;
        catch java.lang.Throwable from label8 to label9 with label8;
    }

    private static void makePartialVectors(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, java.lang.Long, java.lang.Long, int, long, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean, boolean) throws java.io.IOException, java.lang.InterruptedException, java.lang.ClassNotFoundException
    {
        org.apache.hadoop.fs.Path r0, r4, r5;
        org.apache.hadoop.conf.Configuration r1, $r8;
        java.lang.Long r2, r3;
        int i0;
        long l1, $l2, $l3;
        boolean z0, z1, z2;
        java.net.URI $r9;
        java.lang.StringBuilder $r10, $r12, $r13, $r15, $r17;
        org.apache.hadoop.mapreduce.Job $r11;
        java.lang.String $r14, $r16;
        org.apache.hadoop.fs.Path[] $r18, $r19;
        java.lang.IllegalStateException $r20;

        r0 := @parameter0: org.apache.hadoop.fs.Path;

        r1 := @parameter1: org.apache.hadoop.conf.Configuration;

        r2 := @parameter2: java.lang.Long;

        r3 := @parameter3: java.lang.Long;

        i0 := @parameter4: int;

        l1 := @parameter5: long;

        r4 := @parameter6: org.apache.hadoop.fs.Path;

        r5 := @parameter7: org.apache.hadoop.fs.Path;

        z0 := @parameter8: boolean;

        z1 := @parameter9: boolean;

        $r8 = new org.apache.hadoop.conf.Configuration;

        specialinvoke $r8.<org.apache.hadoop.conf.Configuration: void <init>(org.apache.hadoop.conf.Configuration)>(r1);

        virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String)>("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization,org.apache.hadoop.io.serializer.WritableSerialization");

        $l2 = virtualinvoke r2.<java.lang.Long: long longValue()>();

        virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: void setLong(java.lang.String,long)>("feature.count", $l2);

        $l3 = virtualinvoke r3.<java.lang.Long: long longValue()>();

        virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: void setLong(java.lang.String,long)>("vector.count", $l3);

        virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: void setInt(java.lang.String,int)>("min.df", i0);

        virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: void setLong(java.lang.String,long)>("max.df", l1);

        virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: void setBoolean(java.lang.String,boolean)>("vector.sequentialAccess", z0);

        virtualinvoke $r8.<org.apache.hadoop.conf.Configuration: void setBoolean(java.lang.String,boolean)>("vector.named", z1);

        $r9 = virtualinvoke r4.<org.apache.hadoop.fs.Path: java.net.URI toUri()>();

        staticinvoke <org.apache.hadoop.filecache.DistributedCache: void addCacheFile(java.net.URI,org.apache.hadoop.conf.Configuration)>($r9, $r8);

        $r11 = new org.apache.hadoop.mapreduce.Job;

        specialinvoke $r11.<org.apache.hadoop.mapreduce.Job: void <init>(org.apache.hadoop.conf.Configuration)>($r8);

        $r10 = new java.lang.StringBuilder;

        specialinvoke $r10.<java.lang.StringBuilder: void <init>()>();

        $r13 = virtualinvoke $r10.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(": MakePartialVectors: input-folder: ");

        $r12 = virtualinvoke $r13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r0);

        $r15 = virtualinvoke $r12.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(", dictionary-file: ");

        $r14 = virtualinvoke r4.<org.apache.hadoop.fs.Path: java.lang.String toString()>();

        $r17 = virtualinvoke $r15.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($r14);

        $r16 = virtualinvoke $r17.<java.lang.StringBuilder: java.lang.String toString()>();

        virtualinvoke $r11.<org.apache.hadoop.mapreduce.Job: void setJobName(java.lang.String)>($r16);

        virtualinvoke $r11.<org.apache.hadoop.mapreduce.Job: void setJarByClass(java.lang.Class)>(class "org/apache/mahout/vectorizer/tfidf/TFIDFConverter");

        virtualinvoke $r11.<org.apache.hadoop.mapreduce.Job: void setOutputKeyClass(java.lang.Class)>(class "org/apache/hadoop/io/Text");

        virtualinvoke $r11.<org.apache.hadoop.mapreduce.Job: void setOutputValueClass(java.lang.Class)>(class "org/apache/mahout/math/VectorWritable");

        $r19 = newarray (org.apache.hadoop.fs.Path)[1];

        $r19[0] = r0;

        staticinvoke <org.apache.hadoop.mapreduce.lib.input.FileInputFormat: void setInputPaths(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path[])>($r11, $r19);

        staticinvoke <org.apache.hadoop.mapreduce.lib.output.FileOutputFormat: void setOutputPath(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path)>($r11, r5);

        virtualinvoke $r11.<org.apache.hadoop.mapreduce.Job: void setMapperClass(java.lang.Class)>(class "org/apache/hadoop/mapreduce/Mapper");

        virtualinvoke $r11.<org.apache.hadoop.mapreduce.Job: void setInputFormatClass(java.lang.Class)>(class "org/apache/hadoop/mapreduce/lib/input/SequenceFileInputFormat");

        virtualinvoke $r11.<org.apache.hadoop.mapreduce.Job: void setReducerClass(java.lang.Class)>(class "org/apache/mahout/vectorizer/tfidf/TFIDFPartialVectorReducer");

        virtualinvoke $r11.<org.apache.hadoop.mapreduce.Job: void setOutputFormatClass(java.lang.Class)>(class "org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat");

        $r18 = newarray (org.apache.hadoop.fs.Path)[1];

        $r18[0] = r5;

        staticinvoke <org.apache.mahout.common.HadoopUtil: void delete(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path[])>($r8, $r18);

        z2 = virtualinvoke $r11.<org.apache.hadoop.mapreduce.Job: boolean waitForCompletion(boolean)>(1);

        if z2 != 0 goto label1;

        $r20 = new java.lang.IllegalStateException;

        specialinvoke $r20.<java.lang.IllegalStateException: void <init>(java.lang.String)>("Job failed!");

        throw $r20;

     label1:
        return;
    }

    private static void startDFCounting(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.lang.InterruptedException, java.lang.ClassNotFoundException
    {
        org.apache.hadoop.fs.Path r0, r1;
        org.apache.hadoop.conf.Configuration r2, $r5;
        boolean z0;
        org.apache.hadoop.mapreduce.Job $r6;
        java.lang.StringBuilder $r7, $r8, $r9;
        java.lang.String $r10;
        org.apache.hadoop.fs.Path[] $r11, $r12;
        java.lang.IllegalStateException $r13;

        r0 := @parameter0: org.apache.hadoop.fs.Path;

        r1 := @parameter1: org.apache.hadoop.fs.Path;

        r2 := @parameter2: org.apache.hadoop.conf.Configuration;

        $r5 = new org.apache.hadoop.conf.Configuration;

        specialinvoke $r5.<org.apache.hadoop.conf.Configuration: void <init>(org.apache.hadoop.conf.Configuration)>(r2);

        virtualinvoke $r5.<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String)>("io.serializations", "org.apache.hadoop.io.serializer.JavaSerialization,org.apache.hadoop.io.serializer.WritableSerialization");

        $r6 = new org.apache.hadoop.mapreduce.Job;

        specialinvoke $r6.<org.apache.hadoop.mapreduce.Job: void <init>(org.apache.hadoop.conf.Configuration)>($r5);

        $r7 = new java.lang.StringBuilder;

        specialinvoke $r7.<java.lang.StringBuilder: void <init>()>();

        $r8 = virtualinvoke $r7.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("VectorTfIdf Document Frequency Count running over input: ");

        $r9 = virtualinvoke $r8.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(r0);

        $r10 = virtualinvoke $r9.<java.lang.StringBuilder: java.lang.String toString()>();

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setJobName(java.lang.String)>($r10);

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setJarByClass(java.lang.Class)>(class "org/apache/mahout/vectorizer/tfidf/TFIDFConverter");

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setOutputKeyClass(java.lang.Class)>(class "org/apache/hadoop/io/IntWritable");

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setOutputValueClass(java.lang.Class)>(class "org/apache/hadoop/io/LongWritable");

        $r11 = newarray (org.apache.hadoop.fs.Path)[1];

        $r11[0] = r0;

        staticinvoke <org.apache.hadoop.mapreduce.lib.input.FileInputFormat: void setInputPaths(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path[])>($r6, $r11);

        staticinvoke <org.apache.hadoop.mapreduce.lib.output.FileOutputFormat: void setOutputPath(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path)>($r6, r1);

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setMapperClass(java.lang.Class)>(class "org/apache/mahout/vectorizer/term/TermDocumentCountMapper");

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setInputFormatClass(java.lang.Class)>(class "org/apache/hadoop/mapreduce/lib/input/SequenceFileInputFormat");

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setCombinerClass(java.lang.Class)>(class "org/apache/mahout/vectorizer/term/TermDocumentCountReducer");

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setReducerClass(java.lang.Class)>(class "org/apache/mahout/vectorizer/term/TermDocumentCountReducer");

        virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: void setOutputFormatClass(java.lang.Class)>(class "org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat");

        $r12 = newarray (org.apache.hadoop.fs.Path)[1];

        $r12[0] = r1;

        staticinvoke <org.apache.mahout.common.HadoopUtil: void delete(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path[])>($r5, $r12);

        z0 = virtualinvoke $r6.<org.apache.hadoop.mapreduce.Job: boolean waitForCompletion(boolean)>(1);

        if z0 != 0 goto label1;

        $r13 = new java.lang.IllegalStateException;

        specialinvoke $r13.<java.lang.IllegalStateException: void <init>(java.lang.String)>("Job failed!");

        throw $r13;

     label1:
        return;
    }

    public static void <clinit>()
    {
        <org.apache.mahout.vectorizer.tfidf.TFIDFConverter: java.lang.String WORDCOUNT_OUTPUT_FOLDER> = "df-count";

        <org.apache.mahout.vectorizer.tfidf.TFIDFConverter: java.lang.String VECTOR_OUTPUT_FOLDER> = "partial-vectors-";

        <org.apache.mahout.vectorizer.tfidf.TFIDFConverter: int SEQUENCEFILE_BYTE_OVERHEAD> = 45;

        <org.apache.mahout.vectorizer.tfidf.TFIDFConverter: java.lang.String OUTPUT_FILES_PATTERN> = "part-*";

        <org.apache.mahout.vectorizer.tfidf.TFIDFConverter: int MIN_CHUNKSIZE> = 100;

        <org.apache.mahout.vectorizer.tfidf.TFIDFConverter: int MAX_CHUNKSIZE> = 10000;

        <org.apache.mahout.vectorizer.tfidf.TFIDFConverter: java.lang.String FREQUENCY_FILE> = "frequency.file-";

        <org.apache.mahout.vectorizer.tfidf.TFIDFConverter: java.lang.String DOCUMENT_VECTOR_OUTPUT_FOLDER> = "tfidf-vectors";

        <org.apache.mahout.vectorizer.tfidf.TFIDFConverter: java.lang.String MAX_DF> = "max.df";

        <org.apache.mahout.vectorizer.tfidf.TFIDFConverter: java.lang.String MIN_DF> = "min.df";

        <org.apache.mahout.vectorizer.tfidf.TFIDFConverter: java.lang.String FEATURE_COUNT> = "feature.count";

        <org.apache.mahout.vectorizer.tfidf.TFIDFConverter: java.lang.String VECTOR_COUNT> = "vector.count";

        return;
    }
}
