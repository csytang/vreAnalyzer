public class WordCount2$TokenizerMapper extends org.apache.hadoop.mapreduce.Mapper
{
    private static final org.apache.hadoop.io.IntWritable one;
    private org.apache.hadoop.io.Text word;
    private boolean caseSensitive;
    private java.util.Set patternsToSkip;
    private org.apache.hadoop.conf.Configuration conf;
    private java.io.BufferedReader fis;

    static void <clinit>()
    {
        org.apache.hadoop.io.IntWritable $r0;

        $r0 = new org.apache.hadoop.io.IntWritable;

        specialinvoke $r0.<init>(1);

        WordCount2$TokenizerMapper.one = $r0;

        return;
    }

    public void <init>()
    {
        WordCount2$TokenizerMapper r0;
        org.apache.hadoop.io.Text $r1;
        java.util.HashSet $r2;

        r0 := @this;

        specialinvoke r0.<init>();

        $r1 = new org.apache.hadoop.io.Text;

        specialinvoke $r1.<init>();

        r0.word = $r1;

        $r2 = new java.util.HashSet;

        specialinvoke $r2.<init>();

        r0.patternsToSkip = $r2;

        return;
    }

    public void setup(org.apache.hadoop.mapreduce.Mapper$Context) throws java.io.IOException, java.lang.InterruptedException
    {
        WordCount2$TokenizerMapper r0;
        org.apache.hadoop.mapreduce.Mapper$Context r1;
        java.net.URI[] r2;
        java.net.URI r3;
        int i0, i1;
        java.lang.String r6, $r13, $r14;
        org.apache.hadoop.conf.Configuration $r7, $r8, $r9, $r10;
        boolean $z0, $z1;
        org.apache.hadoop.mapreduce.Job $r11;
        org.apache.hadoop.fs.Path $r12;

        r0 := @this;

        r1 := @parameter0;

        $r7 = r1.getConfiguration();

        r0.conf = $r7;

        $r8 = r0.conf;

        $z0 = $r8.getBoolean("wordcount.case.sensitive", 1);

        r0.caseSensitive = $z0;

        $r9 = r0.conf;

        $z1 = $r9.getBoolean("wordcount.skip.patterns", 1);

        if $z1 == 0 goto label3;

        $r10 = r0.conf;

        $r11 = org.apache.hadoop.mapreduce.Job.getInstance($r10);

        r2 = $r11.getCacheFiles();

        i0 = lengthof r2;

        i1 = 0;

        goto label2;

     label1:
        r3 = r2[i1];

        $r12 = new org.apache.hadoop.fs.Path;

        $r13 = r3.getPath();

        specialinvoke $r12.<init>($r13);

        $r14 = $r12.getName();

        r6 = $r14.toString();

        specialinvoke r0.parseSkipFile(r6);

        i1 = i1 + 1;

     label2:
        if i1 < i0 goto label1;

     label3:
        return;
    }

    private void parseSkipFile(java.lang.String)
    {
        WordCount2$TokenizerMapper r0;
        java.lang.String r1, $r5, $r10, $r12;
        java.io.BufferedReader $r2, $r4;
        java.io.FileReader $r3;
        java.util.Set $r6;
        java.io.IOException $r7;
        java.lang.StringBuilder $r8, $r11;
        java.io.PrintStream $r9;

        r0 := @this;

        r1 := @parameter0;

     label1:
        $r2 = new java.io.BufferedReader;

        $r3 = new java.io.FileReader;

        specialinvoke $r3.<init>(r1);

        specialinvoke $r2.<init>($r3);

        r0.fis = $r2;

        goto label3;

     label2:
        $r6 = r0.patternsToSkip;

        $r6.add($r5);

     label3:
        $r4 = r0.fis;

        $r5 = $r4.readLine();

        if $r5 != null goto label2;

     label4:
        goto label6;

     label5:
        $r7 := @caughtexception;

        $r9 = java.lang.System.err;

        $r8 = new java.lang.StringBuilder;

        specialinvoke $r8.<init>("Caught exception while parsing the cached file \'");

        $r10 = org.apache.hadoop.util.StringUtils.stringifyException($r7);

        $r11 = $r8.append($r10);

        $r12 = $r11.toString();

        $r9.println($r12);

     label6:
        return;

        catch java.io.IOException from label1 to label4 with label5;
    }

    public void map(java.lang.Object, org.apache.hadoop.io.Text, org.apache.hadoop.mapreduce.Mapper$Context) throws java.io.IOException, java.lang.InterruptedException
    {
        WordCount2$TokenizerMapper r0;
        java.lang.Object r1, $r6;
        org.apache.hadoop.io.Text r2, $r8, $r11;
        org.apache.hadoop.mapreduce.Mapper$Context r3;
        boolean $z0, $z1, $z2;
        java.lang.String $r4, $r9, $r13, $r15, $r16, r17, r19;
        java.util.Set $r5;
        java.util.StringTokenizer $r7;
        org.apache.hadoop.io.IntWritable $r10;
        java.lang.Class $r12;
        WordCount2$TokenizerMapper$CountersEnum $r14;
        java.util.Iterator r18;
        org.apache.hadoop.mapreduce.Counter r21;

        r0 := @this;

        r1 := @parameter0;

        r2 := @parameter1;

        r3 := @parameter2;

        $z0 = r0.caseSensitive;

        if $z0 == 0 goto label1;

        $r16 = r2.toString();

        goto label2;

     label1:
        $r4 = r2.toString();

        $r16 = $r4.toLowerCase();

     label2:
        r17 = $r16;

        $r5 = r0.patternsToSkip;

        r18 = $r5.iterator();

        goto label4;

     label3:
        $r6 = r18.next();

        r19 = (java.lang.String) $r6;

        r17 = r17.replaceAll(r19, "");

     label4:
        $z1 = r18.hasNext();

        if $z1 != 0 goto label3;

        $r7 = new java.util.StringTokenizer;

        specialinvoke $r7.<init>(r17);

        goto label6;

     label5:
        $r8 = r0.word;

        $r9 = $r7.nextToken();

        $r8.set($r9);

        $r11 = r0.word;

        $r10 = WordCount2$TokenizerMapper.one;

        r3.write($r11, $r10);

        $r12 = class "WordCount2$TokenizerMapper$CountersEnum";

        $r13 = $r12.getName();

        $r14 = WordCount2$TokenizerMapper$CountersEnum.INPUT_WORDS;

        $r15 = $r14.toString();

        r21 = r3.getCounter($r13, $r15);

        r21.increment(1L);

     label6:
        $z2 = $r7.hasMoreTokens();

        if $z2 != 0 goto label5;

        return;
    }

    public volatile void map(java.lang.Object, java.lang.Object, org.apache.hadoop.mapreduce.Mapper$Context) throws java.io.IOException, java.lang.InterruptedException
    {
        WordCount2$TokenizerMapper r0;
        java.lang.Object r1, r2, $r6;
        org.apache.hadoop.mapreduce.Mapper$Context r3, $r4;
        org.apache.hadoop.io.Text $r5;

        r0 := @this;

        r1 := @parameter0;

        r2 := @parameter1;

        r3 := @parameter2;

        $r6 = (java.lang.Object) r1;

        $r5 = (org.apache.hadoop.io.Text) r2;

        $r4 = (org.apache.hadoop.mapreduce.Mapper$Context) r3;

        r0.map($r6, $r5, $r4);

        return;
    }
}
